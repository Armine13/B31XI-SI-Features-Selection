{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pattern recognition - MsCV ViBOT"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Guillaume Lemaitre - Fabrice Meriaudeau - Joan Massich"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "%matplotlib inline\n",
      "%pprint off\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "import plotly.plotly as py\n",
      "from plotly.graph_objs import *\n",
      "py.sign_in('glemaitre', 'se04g0bmi2')\n",
      "\n",
      "from scipy.stats import ttest_ind\n",
      "from scipy.stats.mstats import betai\n",
      "\n",
      "import numpy as np\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Features selection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Student's t-test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Student's t-test allow to determine if two given sets are significantly different from each other. \n",
      "\n",
      "In our case, we want to determine if the different classes composing a single feature dimension are significantly different. To do so, we will test whether the mean values of both classes in a feature dimension will differ significantly. Assuming that the data in the classes are normally distributed (and an assumption about the variance), the so-called t-test is a popular choice.\n",
      "\n",
      "The goal of the t-test is to determine which of the following hypothesis is true:\n",
      "\n",
      "* H1: the mean values of the feature in the two classes are different (*alternative hypothesis*),\n",
      "* H0: the mean values of the feature in the two class are equal (*null hypothesis*).\n",
      "\n",
      "If the *null hypothesis* holds true, the feature is discarded since that the classes cannot be easily separeted. If the *alternative hypothesis* holds true, the feature is selected since that the classes are easily separable. The hypothesis test is carried out against the so-called significance level, $\\rho$, which corresponds to the probability of committing an error in our decision. Typical values are $\\rho=0.05$ and $\\rho= 0.001$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Student's distribution and p-value"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function allows to return the probability of committing an error and is none as p-value. It can be computed using the [regularized incomplete beta function](http://en.wikipedia.org/wiki/Student%27s_t-distribution#Cumulative_distribution_function)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Define a function to compute the probability for a student's distribution\n",
      "def StudentDistribution(t_value, deg_freedom):\n",
      "    return np.squeeze(betai(0.5 * deg_freedom, 0.5, deg_freedom/(deg_freedom + t_value*t_value)).reshape(t_value.shape))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Statistical t-test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given two classes $x_1$ and $x_2$ as well as the significance level $\\rho$, the test will return if the *null hypothesis* is rejected or not. We assume equal number of samples in each class and a shared covariance between the two classes.\n",
      "\n",
      "The following steps have to be performed.\n",
      "\n",
      "* Estimate the mean $\\bar{x_1}$ and $\\bar{x_2}$ for each class,\n",
      "* Estimate the common (assumption) covariance such that $s_{x_1,x_2} = \\sqrt{\\frac{1}{2} (s_{x_1}^{2} + s_{x_2}^{2}})$,\n",
      "* Compute the t-value such that $t = \\frac{\\bar{x_1} - \\bar{x_2}}{s_{x_1,x_2} \\sqrt{\\frac{2}{n}}}$ with $n$ the number of samples in one class,\n",
      "* Compute the degree of freedom $\\nu$ such that $\\nu = 2 n - 2$,\n",
      "* Given the t-value and the degree of freedom, compute the p-value assuming a Student's distribution\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(a) Complete the following code in order to implement the t-test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Definition of a function to make t-test\n",
      "def t_test(x1, x2, p):\n",
      "    \n",
      "    # Estimate the mean and variance\n",
      "    ### Use np.mean()\n",
      "    est_mean_1 = ...\n",
      "    est_mean_2 = ...\n",
      "    ### Use np.std()\n",
      "    est_var = ...\n",
      "    \n",
      "    # Compute the t-value\n",
      "    t = ...\n",
      "    \n",
      "    # Define the degree of freedom\n",
      "    df = ...\n",
      "    \n",
      "    # Compute the probability\n",
      "    p_value = StudentDistribution(t, df)\n",
      "    \n",
      "    return (p_value > p)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Experiment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(b) Repeat 5 times the following experiment:\n",
      "\n",
      "* Generate two classes such that:\n",
      "\n",
      "    * Class #1 follows a normal distribution $\\mathcal{N}(8.8, 4.)$ with 1000 samples,\n",
      "    * Class #2 follows a normal distribution $\\mathcal{N}(9, 4.)$ with 1000 samples.\n",
      "\n",
      "* Plot the distrbution of each class,\n",
      "* Apply the Student's t-test for significance level of $\\rho=0.05$ and $\\rho= 0.001$ and check if the *null hypothesis is rejected or not.\n",
      "\n",
      "For your information, you can find an example of implementation of the Student's t-test using Scipy library which you can use for your future experimentations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "for i in range(0, 5):\n",
      "    \n",
      "    # Declare the mean and std of the different distribution\n",
      "    mean1, mean2, std = ...\n",
      "    \n",
      "    # Define the number of points for each distribution\n",
      "    n_samples = ...\n",
      "    \n",
      "    # Generate the samples for each class\n",
      "    ### Use np.random.normal()\n",
      "    class_1 = ...\n",
      "    class_2 = ...\n",
      "    \n",
      "    # Plot the different class\n",
      "    nb_bins = 50\n",
      "    \n",
      "    fig = plt.figure()\n",
      "    # Plot the PDF of the first class\n",
      "    n, bins, patches = plt.hist(class_1, nb_bins, normed=1, histtype='stepfilled', label='Class #1')\n",
      "    plt.setp(patches, 'facecolor', 'b', 'alpha', 0.75)\n",
      "    # Plot the PDF of the second class\n",
      "    n, bins, patches = plt.hist(class_2, nb_bins, normed=1, histtype='stepfilled', label='Class #2')\n",
      "    plt.setp(patches, 'facecolor', 'r', 'alpha', 0.75)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    \n",
      "    # Check if the null hypothesis is rejected or not\n",
      "    print 'The null hypothesis is {} with p=0.001'.format(t_test(class_1, class_2, 0.001))\n",
      "    print 'The null hypothesis is {} with p=0.05'.format(t_test(class_1, class_2, 0.05))\n",
      "    \n",
      "    # Implementation in scipy\n",
      "    [scipy_t_value, scipy_p_value] = ttest_ind(class_1, class_2)\n",
      "    print 'The statistic computed directly with scipy implementation are: t-value={} and p-value={}'.\\\n",
      "    format(scipy_t_value, scipy_p_value)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Fisher Discriminant Ratio\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Fisher's Discriminant Ratio (FDR) is commonly employed to quantify the discriminatory power of individual features between two equiprobable classes. In other word, it is independent of the type of the classe distribution.\n",
      "\n",
      "For a given feature dimension, the FDR for two classes $x_1$ and $x_2$ is defined as:\n",
      "$$FDR = \\frac{(\\bar{x_1} - \\bar{x_2})^2}{\\sigma_{1}^{2} + \\sigma_{2}^{2}}$$\n",
      "\n",
      "(a) Complete the following Python function in order to compute the FDR for each feature dimension of the class $x_{1}$ and $x_{2}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Compute the Fisher discriminant ratio \n",
      "def FisherDiscriminantRatio(x1, x2):\n",
      "    \n",
      "    fdr = np.ravel(np.zeros((np.shape(x1)[1], 1)))\n",
      "    # Compute the ratio for the different feature dimension\n",
      "    for dim in range(0, np.shape(x1)[1]):\n",
      "        \n",
      "        fdr[dim] = ...\n",
      "    \n",
      "    return fdr\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Synthetic example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(b) Generate two classes such that:\n",
      "\n",
      "* Class #1 follows a multivariate Gaussian distribution with:\n",
      "\n",
      "$$\\mu_1 = \\left[ \\begin{matrix} 1 & 1 & 0 & 6 & 3 \\end{matrix} \\right],$$\n",
      "\n",
      "$$\\Sigma_1 = \\left[ \\begin{matrix} 0.06 & 0 & 0 & 0 & 0 \\\\ 0 & 0.5 & 0 & 0 & 0 \\\\ 0 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 0 & 0.001 & 0 \\\\ 0 & 0 & 0 & 0 & 3 \\end{matrix} \\right].$$\n",
      "\n",
      "* Class #2 follows a multivariate Gaussian distribution with:\n",
      "\n",
      "$$\\mu_2 = \\left[ \\begin{matrix} 11.5 & 11 & 10 & 6.5 & 4 \\end{matrix} \\right],$$\n",
      "\n",
      "$$\\Sigma_2 = \\left[ \\begin{matrix} 0.06 & 0 & 0 & 0 & 0 \\\\ 0 & 0.6 & 0 & 0 & 0 \\\\ 0 & 0 & 34 & 0 & 0 \\\\ 0 & 0 & 0 & 0.001 & 0 \\\\ 0 & 0 & 0 & 0 & 4 \\end{matrix} \\right].$$\n",
      "\n",
      "* Generate 500 samples for each class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Define the mean and covariance\n",
      "### Use np.array() and np.diagflat()\n",
      "mean1, mean2 = ...\n",
      "std1, std2 = ...\n",
      "\n",
      "# Define the number of data in each class\n",
      "n_samples = ...\n",
      "\n",
      "# Generate the data\n",
      "### Use np.random.multivariate_normal()\n",
      "class_1 = ...\n",
      "class_2 = ...\n",
      "\n",
      "# Plot the different distribution for each dimension\n",
      "nb_bins = 10\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111, projection='3d')\n",
      "for dim in range(0, np.size(mean1)):\n",
      "    # Plot the PDF of the class #1\n",
      "    bins, n = np.histogram(class_1[:, dim], nb_bins, normed = False)\n",
      "    ax.bar(n[0:-1], bins, (dim * 10), zdir='y', color='b', alpha=0.75, width=n[1]-n[0], edgecolor='none')\n",
      "    # Plot the PDF of the class #2\n",
      "    bins, n = np.histogram(class_2[:, dim], nb_bins, normed = False)\n",
      "    ax.bar(n[0:-1], bins, (dim * 10) + 1, zdir='y', color='r', alpha=0.75, width=n[1]-n[0], edgecolor='none')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(c) Compute the FDR for the above generated classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Compute the Fisher ratio\n",
      "### Use the function implemented FisherDiscriminantRatio()\n",
      "fisher_ratio = ...\n",
      "\n",
      "# Find the most discriminative feature dimension\n",
      "### Use np.argsort() to know which feature dimension are the most important\n",
      "print 'The feature dimension can be ordered as {}'.format(np.argsort(fisher_ratio))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Real dataset: UCI liver disorders dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this second example, we consider the data of the [UCI liver disorders dataset](https://archive.ics.uci.edu/ml/datasets/Liver+Disorders). Necessary information about the data are available in the following Python cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Attribute information:\n",
      "###   0. mcv\t    mean corpuscular volume\n",
      "###   1. alkphos\talkaline phosphotase\n",
      "###   2. sgpt\t    alamine aminotransferase\n",
      "###   3. sgot \t    aspartate aminotransferase\n",
      "###   4. gammagt\tgamma-glutamyl transpeptidase\n",
      "###   5. drinks\t    number of half-pint equivalents of alcoholic beverages drunk per day\n",
      "###   6. selector   field used to split data into two sets\n",
      "\n",
      "# Relevant information:\n",
      "###   -- The first 5 variables are all blood tests which are thought\n",
      "###      to be sensitive to liver disorders that might arise from\n",
      "###      excessive alcohol consumption.  Each line in the bupa.data file\n",
      "###      constitutes the record of a single male individual.\n",
      "###   -- It appears that drinks>5 is some sort of a selector on this database.\n",
      "\n",
      "# Load the data/bupa.data\n",
      "data = np.loadtxt('./data/bupa.data', delimiter=',')\n",
      "\n",
      "# Split into data and label\n",
      "### Label are the two clas which is either 0 or 1\n",
      "label = np.ravel(data[:, -1] - 1)\n",
      "data = np.asmatrix(data[:, 0 : -1])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(d) Compute the FDR for the above generated classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Compute the Fisher Discriminant Ratio\n",
      "fisher_ratio = ...\n",
      "\n",
      "# Find the most discriminative feature dimension\n",
      "### Use np.argsort() to know which feature dimension are the most important\n",
      "print 'The feature dimension can be ordered as {}'.format(np.argsort(fisher_ratio))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Measures based on scatter matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define the following Python class 'pr_class' which will be used afterword."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Define a class for all the information\n",
      "class pr_class(object):\n",
      "    def __init__(self, X, y):\n",
      "        self.n_dims = np.shape(X)[1]\n",
      "        self.n_samples = np.shape(X)[0]\n",
      "        self.data = X\n",
      "        self.gt = y\n",
      "        self.mean_vec = np.mean(self.data, axis = 0)\n",
      "        self.cov_mat = (self.data - self.mean_vec).T * (self.data - self.mean_vec) / (float(self.n_samples) - 1.)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scatter matrices are among the most popular measures for quantifying the way feature vectors \u201cscatter\u201d in the feature space. Because of their rich physical meaning, a number of class-separability measures are built around them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Three such measures are defined as:\n",
      "\n",
      "$$J_1 = \\frac{\\text{tr}(S_m)}{\\text{tr}(S_w)},$$\n",
      "\n",
      "$$J_2 = \\frac{\\det(S_m)}{\\det(S_w)},$$\n",
      "\n",
      "$$J_3 = \\text{tr}(S_w^{-1} S_m).$$\n",
      "\n",
      "(a) Complete the following Python functions to implement the measures $J_1$, $J_2$ and $J_3$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Define the different scatter measure\n",
      "### Use np.trace()\n",
      "### Use np.linalg.det()\n",
      "### Use np.linalg.inv()\n",
      "\n",
      "def J1(Sm, Sw):\n",
      "    return ...\n",
      "\n",
      "def J2(Sm, Sw):\n",
      "    return ...\n",
      "\n",
      "def J3(Sm, Sw):\n",
      "    return ...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(b) Thus, we need to compute the scatter matrices:\n",
      "\n",
      "* $S_m$ is the covariance matrix for the whole data. The concatenation is performed as an example for the future example,\n",
      "* $S_b$ corresponds to the sum of the within class variance normalise by the number of classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "##### X will be a list of 'pr_class' object #####\n",
      "\n",
      "# Definition of the scatter mixture matrix\n",
      "def ComputeSm(X):\n",
      "    # Concatenate all the data to get the mixture covariance\n",
      "    all_data = np.squeeze([X[count].data for count in xrange(np.size(X))])\n",
      "    all_data = np.concatenate([all_data[count] for count in xrange(np.shape(all_data)[0])], axis=0)\n",
      "    ### Return using np.asmatrix() and using np.cov()\n",
      "    return ...\n",
      "\n",
      "# Definition of the scatter within class matrix\n",
      "### Check the attribute pr_class.cov_mat\n",
      "def ComputeSw(X):\n",
      "    Sw = 0.\n",
      "    for c in range(0, np.size(X)):\n",
      "        Sw += ...\n",
      "    Sw /= ...\n",
      "    ### Return using np.asmatrix()\n",
      "    return ...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use the UCI liver disorders dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Attribute information:\n",
      "###   0. mcv\t    mean corpuscular volume\n",
      "###   1. alkphos\talkaline phosphotase\n",
      "###   2. sgpt\t    alamine aminotransferase\n",
      "###   3. sgot \t    aspartate aminotransferase\n",
      "###   4. gammagt\tgamma-glutamyl transpeptidase\n",
      "###   5. drinks\t    number of half-pint equivalents of alcoholic beverages drunk per day\n",
      "###   6. selector   field used to split data into two sets\n",
      "\n",
      "# Relevant information:\n",
      "###   -- The first 5 variables are all blood tests which are thought\n",
      "###      to be sensitive to liver disorders that might arise from\n",
      "###      excessive alcohol consumption.  Each line in the bupa.data file\n",
      "###      constitutes the record of a single male individual.\n",
      "###   -- It appears that drinks>5 is some sort of a selector on this database.\n",
      "\n",
      "# Load the data/bupa.data\n",
      "data = np.loadtxt('./data/bupa.data', delimiter=',')\n",
      "\n",
      "# Split into data and label\n",
      "### Label are the two clas which is either 0 or 1\n",
      "label = np.ravel(data[:, -1]) - 1\n",
      "data = np.asmatrix(data[:, 0 : -1])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(c) Normalise the dataset such that they have a Normal distribution $\\mathcal{N}(0, 1)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Normalise the data using mean and std\n",
      "### Compute the mean vector\n",
      "mean_vec = ...\n",
      "### Compute the std vector\n",
      "std_vec = ...\n",
      "### Normalise the data\n",
      "data_norm = ...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are generating all the possible combination of 3 features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from itertools import combinations\n",
      "\n",
      "# Generate a vector with all the combination with 3 dimension only\n",
      "iter_comb = combinations(range(6), 3)\n",
      "all_comb = []\n",
      "for cb in iter_comb:\n",
      "    all_comb.append(cb)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(d) Compute the the different measures for each combination of feature previosuly generated:\n",
      "\n",
      "* Define the data subset depending of the current combination,\n",
      "* Create a list of 'pr_class',\n",
      "* Compute the matrices $S_m$ and $S_b$,\n",
      "* Compute the measures $J_1$,$J_2$ and $J_3$.\n",
      "* Show the best combination of feature for each measure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "  \n",
      "J_mes = []\n",
      "for cb in range(0, np.shape(all_comb)[0]):\n",
      "    \n",
      "    # Find the number of classes\n",
      "    ### Use np.unique()\n",
      "    nb_classes = ...\n",
      "    # Define the data subset depending of the current combination\n",
      "    data_sub = ...\n",
      "    \n",
      "    # Create an object for each class\n",
      "    classes_pr = ...\n",
      "    \n",
      "    # Compute the different sccatter\n",
      "    Sw_mat = ...\n",
      "    Sm_mat = ...\n",
      "    \n",
      "    # Compute the different measure\n",
      "    j1_meas = ...\n",
      "    j2_meas = ...\n",
      "    j3_meas = ...\n",
      "    \n",
      "    J_mes.append([j1_meas, j2_meas, j3_meas]);\n",
      "\n",
      "# I prefer matrix\n",
      "J_mes = np.asmatrix(J_mes)\n",
      "\n",
      "# Find the best combination\n",
      "print 'The best feature combination taking into account J1 is {}'.format(all_comb[np.argmax(J_mes[:, 0])])\n",
      "print 'The best feature combination taking into account J2 is {}'.format(all_comb[np.argmax(J_mes[:, 1])])\n",
      "print 'The best feature combination taking into account J3 is {}'.format(all_comb[np.argmax(J_mes[:, 2])])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We plot the data in the best combination of 3 features found."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Define the parameters for the class #1\n",
      "trace1 = Scatter3d(\n",
      "    x=np.ravel(data_norm[np.ravel(np.nonzero(label == 0.)), all_comb[np.argmax(J_mes[:, 0])][0]]),\n",
      "    y=np.ravel(data_norm[np.ravel(np.nonzero(label == 0.)), all_comb[np.argmax(J_mes[:, 0])][1]]),\n",
      "    z=np.ravel(data_norm[np.ravel(np.nonzero(label == 0.)), all_comb[np.argmax(J_mes[:, 0])][2]]),\n",
      "    name='Class #1',\n",
      "    mode='markers', marker=Marker(size=5, line=Line(color='rgba(0,0,255,1.0)', width=0.5), opacity=.8))\n",
      "# Define the parameters for the class #2\n",
      "trace2 = Scatter3d(\n",
      "    x=np.ravel(data_norm[np.ravel(np.nonzero(label == 1.)), all_comb[np.argmax(J_mes[:, 0])][0]]),\n",
      "    y=np.ravel(data_norm[np.ravel(np.nonzero(label == 1.)), all_comb[np.argmax(J_mes[:, 0])][1]]),\n",
      "    z=np.ravel(data_norm[np.ravel(np.nonzero(label == 1.)), all_comb[np.argmax(J_mes[:, 0])][2]]),\n",
      "    name='Class #2',\n",
      "    mode='markers', marker=Marker(size=5, symbol='diamond', line=Line(color='rgba(255,0,0,1.0)', width=0.5), opacity=.8))\n",
      "# Concatenate the data\n",
      "plot_data = Data([trace1, trace2])\n",
      "# Define the layout\n",
      "layout = Layout(margin=Margin(l=0, r=0, b=0, t=0))\n",
      "# Define a figure\n",
      "fig = Figure(data=plot_data, layout=layout)\n",
      "py.iplot(fig, filename='projection-fdr')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Subset searchig algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will investigate the suboptimal searching techniques/the exhaustive search."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(a) Using the normalised data of the previous exercise, order them using the FDR."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Order the data depending of the FDR\n",
      "\n",
      "# Compute the Fisher Discriminant Ratio\n",
      "fisher_ratio = ...\n",
      "\n",
      "# Find the most discriminative feature dimension\n",
      "print 'The feature dimension can be ordered as {}'.format(np.argsort(fisher_ratio))\n",
      "\n",
      "data_norm_sorted = data_norm[:, np.argsort(fisher_ratio)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Sequential Forward Selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Apply the sequential forward selection\n",
      "def SequentialForwardSelection(data, label, nb_fea_wanted, metric):\n",
      "       \n",
      "    k = 1\n",
      "    cLBest = []\n",
      "    NofFeatures=np.shape(data)[1];\n",
      "    while k <= nb_fea_wanted:\n",
      "        maxJ = 0\n",
      "        for i in range(0, NofFeatures):\n",
      "            if (np.size(np.nonzero(cLBest == i)) == 0):\n",
      "                combi = cLBest\n",
      "                combi = np.append(combi, i).astype(int)\n",
      "            else:\n",
      "                continue\n",
      "                \n",
      "            # Select the data\n",
      "            data_sub = data_norm[:, combi]\n",
      "            \n",
      "            # Find the number of classes\n",
      "            nb_classes = np.size(np.unique(label))\n",
      "            # Create an object for each class\n",
      "            classes_pr = [pr_class(data_sub[np.ravel(np.nonzero(label == (np.unique(label)[count]))), :], \\\n",
      "                                   label[np.ravel(np.nonzero(label == (np.unique(label)[count])))]) \\\n",
      "                          for count in xrange(nb_classes)]\n",
      "            \n",
      "            classes_pr\n",
      "            \n",
      "            # Compute the different sccatter\n",
      "            Sw = ComputeSw(classes_pr)\n",
      "            Sm = ComputeSm(classes_pr)\n",
      "            \n",
      "            # Compute the different measure\n",
      "            if metric == 'J1':\n",
      "                meas = J1(Sm, Sw)\n",
      "            elif metric == 'J2':\n",
      "                meas = J2(Sm, Sw)\n",
      "            elif metric == 'J3':\n",
      "                meas = J3(Sm, Sw)\n",
      "  \n",
      "            if meas > maxJ:\n",
      "                maxJ = meas\n",
      "                sofar=combi\n",
      "                \n",
      "        cLBest = sofar\n",
      "        k += 1\n",
      "        \n",
      "    return (cLBest, maxJ)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(b) Apply the SFS algorithm to select the feature using `J3` as cost function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Apply SFS algorithm\n",
      "feat_sel, metric = ...\n",
      "print 'The feature selected are {}'.format(feat_sel)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(c) Plot the normalised data in the best 3 features dimensions as previously done."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Define the parameters for the class #1\n",
      "trace1 = Scatter3d(\n",
      "    x=...\n",
      "    y=...\n",
      "    z=...\n",
      "    name='Class #1',\n",
      "    mode='markers', marker=Marker(size=5, line=Line(color='rgba(0,0,255,1.0)', width=0.5), opacity=.8))\n",
      "# Define the parameters for the class #2\n",
      "trace2 = Scatter3d(\n",
      "    x=...\n",
      "    y=...\n",
      "    z=...\n",
      "    name='Class #2',\n",
      "    mode='markers', marker=Marker(size=5, symbol='diamond', line=Line(color='rgba(255,0,0,1.0)', width=0.5), opacity=.8))\n",
      "# Concatenate the data\n",
      "plot_data = Data([...])\n",
      "# Define the layout\n",
      "layout = Layout(margin=Margin(l=0, r=0, b=0, t=0))\n",
      "# Define a figure\n",
      "fig = Figure(data=plot_data, layout=layout)\n",
      "py.iplot(fig, filename='projection-sfs')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}